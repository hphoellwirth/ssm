\documentclass[11pt]{beamer}

\mode<presentation> {
\usetheme{Boadilla} }

\usepackage{graphicx} 
\usepackage{booktabs}
\usepackage[utf8]{inputenc}
\graphicspath{ {../../images/} }

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}


%%%%%%%%%%%
%      Title Page      %
%%%%%%%%%%%
\title[Master Project]{Particle Filtering for Nonlinear State Space Models} 
\author[]{Hans-Peter H{\"o}llwirth \\ Supervisor: Christian Brownlees}
\institute[BGSE]{Barcelona Graduate School of Economics} 
\date{\today}

\begin{document}

\frame{\titlepage}

%%% Outline %%%
\begin{frame}
\frametitle{Outline}
\begin{itemize}
\item State Space Models
\item Filtering
	\begin{itemize}
	\item Kalman Filter
	\item Sequential Importance Resampling (SIR)
	\item Continuous Sequential Importance Resampling (CSIR)
	\item Importance Sampling Particle Filter
	\end{itemize}
\item Evaluation
\item Illustration
	\begin{itemize}
	\item Trivariate Local Level Model
	\item Hierarchical Dynamic Poisson Model
	\end{itemize}
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%
%      State Space Models      %
%%%%%%%%%%%%%%%%
\section{State Space Models}

%%% Local Level Model %%%
\begin{frame}
\frametitle{Local Level Model}
\begin{block}{Formulation}
\begin{center}
\begin{tabular}{ r r l }
  observation: & $y_t = x_t + \epsilon_t$, & $\epsilon_t \sim N(0,\sigma_{\epsilon}^2)$ \\
  state: & $x_{t+1} = x_t + \eta_t$, & $\eta_t \sim N(0,\sigma_{\eta}^2)$ \\
\end{tabular}
\end{center}
\end{block}
$$
\boldsymbol{\theta} = [\sigma_{\eta}^2,  \sigma_{\epsilon}^2]^T
$$
\begin{center}
\begin{tabular}{ r r l }
  transition density: & $x_{t+1} | x_t, \boldsymbol{\theta}$ & $\sim N(x_t,\sigma_{\epsilon}^2)$ \\
  measurement density: & $y_t | x_t, \boldsymbol{\theta}$ & $\sim N(x_t,\sigma_{\eta}^2)$ \\
\end{tabular}
\end{center}
\end{frame}

%%% Realization %%%
\begin{frame}
\frametitle{Local Level Realization}
\framesubtitle{$\sigma_{\eta}^2=1.4$, $\sigma_{\epsilon}^2=1.0$}
\centering
\includegraphics[scale=0.45]{ullm-realization}
\end{frame}

%%%%%%%%%%%%%
%      Particle Filtering     %
%%%%%%%%%%%%%
\section{Filtering}

%%% Filtering %%%
\begin{frame}
\frametitle{Filtering}
Let $\mathcal{I}_t = \{y_1, y_2, \ldots, y_t\}$. The objective of filtering is to update our knowledge of the system $p(x_{0:t} | \mathcal{I}_t,\boldsymbol{\theta})$ each time a new observation $y_t$ is brought in. $p(x_{0:t} | \mathcal{I}_t,\boldsymbol{\theta})$ can be decomposed in recursive form:
$$
p(x_{0:t} | \mathcal{I}_t,\boldsymbol{\theta}) = \Big[ \frac{p(y_t | x_t,\boldsymbol{\theta}) p(x_t | x_{t-1},\boldsymbol{\theta})}{p(y_t | \mathcal{I}_{t-1},\boldsymbol{\theta})} \Big] p(x_{0:(t-1)} | \mathcal{I}_{t-1},\boldsymbol{\theta}) 
$$
where
\begin{itemize}
\item $p(y_t | x_t,\boldsymbol{\theta})$ is the measurement density
\item $p(x_t | x_{t-1},\boldsymbol{\theta})$ is the transition density
\end{itemize}
\bigskip
\alert{Particle filtering}: recursively simulate the transition density and evaluate the measurement density 
\end{frame}

%%% Kalman filter %%%
\begin{frame}
\frametitle{Kalman filter}
\framesubtitle{by Kalman (1960)}
For linear Gaussian state space models, $p(x_{0:t} | \mathcal{I}_t,\boldsymbol{\theta})$ is analytically tractable. The \textbf{Kalman filter} infers latent states analytically by recursively updating:
\begin{enumerate}
\item the prediction density $x_{t} | \mathcal{I}_{t-1}, \boldsymbol{\theta} \sim N(\mu_{t | t-1}, \Sigma_{t | t-1})$
\item the filtering density $x_{t} | \mathcal{I}_{t}, \boldsymbol{\theta} \sim N(\mu_{t | t}, \Sigma_{t | t})$
\end{enumerate}
\bigskip
Latent state estimates statistically minimize the error and hence are \textit{optimal}.
\end{frame}

\begin{frame}
\frametitle{Kalman filter}
\framesubtitle{Latent State Inference}
\centering
\includegraphics[scale=0.45]{ullm-estimate-kalman}
\end{frame}

%%% SIR filter %%%
\begin{frame}
\frametitle{Sequential Importance Resampling (SIR)}
\framesubtitle{by Gordon et al. (1993)}
Recursively computes $P$ prediction and filtering particles:
\begin{enumerate}
	\item \textbf{Prediction step}: draw prediction particles from transition density:
	$$
	x_{t | t-1}^i \sim p(x_t | x_{t-1 | t-1}^i, \boldsymbol{\theta}) \quad \text{for } i=1, \ldots, P
	$$
	\item \textbf{Filtering step}: draw filtering particles via multinomial sampling: 	
	$$
	x_{t | t }^j \sim MN(w_t^1, \ldots, w_t^P) \quad \text{for } j=1, \ldots, P
	$$
	where importance weights are (normalized) evaluations of the measurement density 
	$$
	w_{t}^i = \frac{p(y_t | x_{t | t-1}^i, \boldsymbol{\theta})}{\sum_{j=1}^P p(y_t | x_{t | t-1}^j, \boldsymbol{\theta})} \quad \text{for } i=1, \ldots, P
	$$
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Sequential Importance Resampling (SIR)}
\framesubtitle{Latent State Inference}
\centering
\includegraphics[scale=0.45]{ullm-estimate-particle}
\end{frame}

\begin{frame}
\frametitle{Sequential Importance Resampling (SIR)}
\framesubtitle{Parameter Inference}
Maximum likelihood estimation on the approximated log-likelihood of the observations, given all previous observations:
\begin{align*} 
\begin{split}
\log \hat{\mathcal{L}}(\mathcal{I}_T, \boldsymbol{\theta}) &= \log \prod_{t=1}^T \hat{p}(y_t | \mathcal{I}_{t-1}, \boldsymbol{\theta}) \\
&= \sum_{t=1}^T \log \hat{p}(y_t | \mathcal{I}_{t-1}, \boldsymbol{\theta}) \\
&= \sum_{t=1}^T \log \frac{1}{P} \sum_{i=1}^P p(y_t | x_{t | t-1}^i, \boldsymbol{\theta}) \\
\end{split}					
\end{align*} 
\end{frame}

%%% CSIR filter %%%
\begin{frame}
\frametitle{Continuous Sequential Importance Resampling (CSIR)}
\framesubtitle{by Malik \& Pitt (2011)}
\textbf{Problem}: Log-likelihood estimator of SIR method w.r.t. $\boldsymbol{\theta}$ is not smooth. Poor parameter inference results.\\
\bigskip
\textbf{Solution}: Smooth it! Sort prediction particles in ascending order, form (discrete) CDF using associated weights, interpolate CDF, and finally draw filtering particles from this inverted, smoothed CDF.
\end{frame}

\begin{frame}
\framesubtitle{Comparison of log-likelihoods w.r.t. $\sigma_{\eta}^2$}
\frametitle{Continuous Sequential Importance Resampling (CSIR)}
\centering
\includegraphics[scale=0.30]{ullm-loglik-zoom-2}\\
\bigskip
\textbf{Caveat}: CSIR method only works for univariate state space models. 
\end{frame}

%%% Importance Sampling particle filter %%%
\begin{frame}
\frametitle{Importance Sampling Particle Filter}
\framesubtitle{by Brownlees \& Kristensen (2017)}
\textbf{Key idea}: Use an auxiliary, misspecified particle filter with parameter vector $\boldsymbol{\tilde{\theta}}$ to compute the likelihood with respect to $\boldsymbol{\theta}$ via recursive importance sampling.
\begin{align*} 
\begin{split}
p(y_t | \mathcal{I}_{t-1}, \boldsymbol{\theta}) &= \int p(y_t | \tilde{x}_t, \boldsymbol{\theta}) p(\tilde{x}_t | \mathcal{I}_{t-1}, \boldsymbol{\theta}) d \tilde{x}_t \\
&= \int p(y_t | \tilde{x}_t, \boldsymbol{\theta}) \bigg[ \frac{p(\tilde{x}_t | \mathcal{I}_{t-1}, \boldsymbol{\theta})}{p(\tilde{x}_t | \mathcal{I}_{t-1}, \boldsymbol{\tilde{\theta}})} \bigg] p(\tilde{x}_t | \mathcal{I}_{t-1}, \boldsymbol{\tilde{\theta}}) d \tilde{x}_t \\
\hat{p}(y_t | \mathcal{I}_{t-1}, \boldsymbol{\theta}) &\approx \frac{1}{P} \sum_{i=1}^P p(y_t | \tilde{x}_{t|t-1}^i, \boldsymbol{\theta})  \bigg[ \frac{p(\tilde{x}_{t|t-1}^i | \mathcal{I}_{t-1}, \boldsymbol{\theta})}{p(\tilde{x}_{t|t-1}^i | \mathcal{I}_{t-1}, \boldsymbol{\tilde{\theta}})} \bigg] \\
&= \frac{1}{P} \sum_{i=1}^P p(y_t | \tilde{x}_{t|t-1}^i, \boldsymbol{\theta}) is_{t|t-1}^i \\
\end{split}					
\end{align*}  
\end{frame}

\begin{frame}
\frametitle{Importance Sampling Particle Filter}
\framesubtitle{Importance Weights}
Recursively computes $P$ prediction and filtering importance weights:
\begin{enumerate}
	\item \textbf{Prediction importance weight}: compares transition densities
	$$
	is_{t|t-1}^i = \bigg[ \frac{p(\tilde{x}_{t|t-1}^i | \tilde{x}_{t-1|t-1}^i, \boldsymbol{\theta})}{p(\tilde{x}_{t|t-1}^i | \tilde{x}_{t-1|t-1}^i, \boldsymbol{\tilde{\theta}})} \bigg] is_{t-1|t-1}^i	
	$$  
	\item \textbf{Filtering importance weight}: compares measurement densities and likelihood	
	$$
	is_{t|t}^i = \bigg[ \frac{p(y_t | \tilde{x}_{t|t}^i, \boldsymbol{\theta})}{p(y_t | \tilde{x}_{t|t}^i, \boldsymbol{\tilde{\theta}})} \bigg] \bigg[ \frac{p(y_t | \mathcal{I}_{t-1}, \boldsymbol{\tilde{\theta}})}{p(y_t | \mathcal{I}_{t-1}, \boldsymbol{\theta})} \bigg] is_{t|t-1}^j		
	$$ 
	where $j$ is such that $\tilde{x}_{t|t}^i = \tilde{x}_{t|t-1}^j$
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Importance Sampling Particle Filter}
\framesubtitle{Mean Importance Weights}
\centering
\includegraphics[scale=0.45]{ullm_is_filt_weights_P200}\\
\includegraphics[scale=0.45]{ullm_is_pred_weights_P200}
\end{frame}

\begin{frame}
\frametitle{Importance Sampling Particle Filter}
\framesubtitle{Parameter Inference}
The estimated log-likelihood of $\boldsymbol{\theta}$ weighs each auxiliary particle $\tilde{x}_{t|t-1}^i$ with predictive importance weight $is_{t|t-1}^i$: 
\begin{align*} 
\begin{split}
\log \hat{\mathcal{L}}(\mathcal{I}_T, \boldsymbol{\theta}) &= \log \prod_{t=1}^T \hat{p}(y_t | \mathcal{I}_{t-1}, \boldsymbol{\theta}) \\
&= \sum_{t=1}^T \log \hat{p}(y_t | \mathcal{I}_{t-1}, \boldsymbol{\theta}) \\
&= \sum_{t=1}^T \log \frac{1}{P} \sum_{i=1}^P p(y_t | \tilde{x}_{t|t-1}^i, \boldsymbol{\theta}) is_{t|t-1}^i \\
\end{split}					
\end{align*} 
\end{frame}

%%%%%%%%%%%
%      Evaluation     %
%%%%%%%%%%%
\section{Evaluation}

%%% Method Comparison %%%
\begin{frame}
\frametitle{Method Comparison}
\framesubtitle{Log-likelihood plots}
\centering
\includegraphics[scale=0.30]{ullm-loglik-eta}
\includegraphics[scale=0.30]{ullm-loglik-zoom}
\end{frame}

%%% Monte Carlo Simulations %%%
\begin{frame}
\frametitle{Monte Carlo Simulations}
...on random realizations of the local level model with $\sigma_{\eta}^2=1.4$ and $\sigma_{\epsilon}^2=1.0$ of several different lengths $T$ \\(and different values of $P$: 20, 50, 200, 500)
\bigskip
\begin{center}
\includegraphics[scale=0.33]{ullm-mc-mle}
\end{center}
\end{frame}

%%% Method Comparison %%%
\begin{frame}
\frametitle{Method Comparison}
\framesubtitle{Filter Choice}
\begin{table}
\centering
\begin{tabular}{lccl}
\hline
Filter  & Latent state & Parameter & Comment\\
\hline
Kalman    & x & x & linear Gaussian models only\\
SIR      & x & &\\
CSIR      & x & x & univariate models only\\
IS      & & x & \\
\hline
\end{tabular}
\end{table}
\end{frame}

%%%%%%%%%%
%      Illustration     %
%%%%%%%%%%
\section{Illustration}

%%% Trivariate Local Level Model %%%
\begin{frame}
\frametitle{Trivariate Local Level Model}
\begin{block}{Formulation}
\begin{center}
\begin{tabular}{ r r l }
  observation: & $\boldsymbol{y}_t = \boldsymbol{x}_t + \boldsymbol{\epsilon}_t$, & $\boldsymbol{\epsilon}_t \sim N(\textbf{0}, \sigma_{\epsilon}^2 I_3)$ \\
  state: & $\boldsymbol{x}_{t+1} = \boldsymbol{x}_t + \boldsymbol{\eta}_t$, & $\boldsymbol{\eta}_t \sim N(\textbf{0}, \Sigma_{\eta})$ \\
\end{tabular}
\end{center}
\end{block}
$$
\Sigma_{\eta} = 
\begin{bmatrix}
\sigma_{\eta 1}^2 & \rho \sigma_{\eta 1} \sigma_{\eta 2} & \rho \sigma_{\eta 1} \sigma_{\eta 3}
\\ \rho \sigma_{\eta 1} \sigma_{\eta 2} & \sigma_{\eta 2}^2 & \rho \sigma_{\eta 2} \sigma_{\eta 3}
\\ \rho \sigma_{\eta 1} \sigma_{\eta 3} & \rho \sigma_{\eta 2} \sigma_{\eta 3} & \sigma_{\eta 3}^2
\end{bmatrix}
$$
\bigskip
$$
\boldsymbol{\theta} = [\rho, \sigma_{\eta 1}^2, \sigma_{\eta 2}^2, \sigma_{\eta 3}^2, \sigma_{\epsilon}^2]^T
$$
\end{frame}

%%% Realization %%%
\begin{frame}
\frametitle{Trivariate Local Level Realization}
\centering
\includegraphics[scale=0.35]{mllm-realization-v2}\\
\bigskip
$\boldsymbol{\theta} = [\rho = 0.7, \sigma_{\eta 1}^2 = 4.2, \sigma_{\eta 2}^2 = 2.8, \sigma_{\eta 3}^2 = 0.9, \sigma_{\epsilon}^2 = 1.0]^T$
\end{frame}

%%% Latent State Inference %%%
\begin{frame}
\frametitle{Latent State Inference}
\framesubtitle{Kalman filter}
\centering
\includegraphics[scale=0.35]{mllm-estimate-kalman}
\end{frame}

\begin{frame}
\frametitle{Latent State Inference}
\framesubtitle{SIR particle filter}
\centering
\includegraphics[scale=0.35]{mllm-estimate-sir}
\end{frame}

%%% Parameter Inference %%%
\begin{frame}
\frametitle{Parameter Inference}
\framesubtitle{Log-likelihood plots w.r.t. $\sigma_{\eta 2}^2$ and $\rho$}
\centering
\includegraphics[scale=0.33]{mllm-loglik-var2}
\includegraphics[scale=0.33]{mllm-loglik-rho}
\end{frame}

\begin{frame}
\frametitle{Parameter Inference}
\framesubtitle{Results}
\begin{table}
\centering
\begin{tabular}{l r r r r}
\hline
& $\sigma_{\eta 1}^2$ &  $\sigma_{\eta 2}^2$ & $\sigma_{\eta 3}^2$ & $\rho$\\
\hline
True        & 4.20  & 2.80 &  0.90 & 0.70\\
Kalman   & 4.96  & 3.10  & 1.01 & 0.73\\
SIR         & 2.27  & 1.53  & 1.29 & 0.52\\
IS            & 2.69  & 2.09  & 1.06 & 0.42\\
\hline
\end{tabular}
\end{table}
\end{frame}

%%% Hierarchical Dynamic Poisson Model %%%
\begin{frame}
\frametitle{Hierarchical Dynamic Poisson Model}
\begin{block}{Formulation}
\begin{center}
\begin{tabular}{ r r l }
  observation: & $y_{m,n}$ & $\sim \text{Poisson}(\lambda_{m,n})$\\
  state: & $\log \lambda_{m,n}$ & $= \log \lambda_m^{(D)} + \log \lambda_{m,n}^{(I)} + \log \lambda_n^{(P)}$\\  
\end{tabular}
\end{center}
\end{block}
\begin{center}
\begin{tabular}{ r l l l}
  daily:& $\log \lambda_{m+1}^{(D)}$ &$= \phi_0^{(D)} + \phi_1^{(D)} \log \lambda_{m}^{(D)}  + \eta_m^{(D)}$ & $\eta_t \sim N(0, \sigma^2_{(D)})$ \\
  intra-daily:& $\log \lambda_{m,n+1}^{(I)}$ &$= \phi_1^{(I)} \log \lambda_{m,n}^{(I)}  + \eta_{m,n}^{(I)}$ & $\eta_{m,n} \sim N(0, \sigma^2_{(I)})$ \\
    periodic:& $\log \lambda_n^{(P)} $ &$= \phi_1^{(P)} \sin(\pi (n-1)/M)$ &\\
\end{tabular}
\end{center}
\bigskip
$$
\boldsymbol{\theta} = [ \phi_0^{(D)},  \phi_1^{(D)}, \sigma^2_{(D)}, \phi_1^{(I)}, \sigma^2_{(I)}, \phi_1^{(P)}]^T
$$
\end{frame}

%%% Realization %%%
\begin{frame}
\frametitle{Hierarchical Dynamic Poisson Realization}
\framesubtitle{$N=5$, $M=20$}
\centering
\includegraphics[scale=0.45]{hdpm-realization.png}\\
\bigskip
\small
$\boldsymbol{\theta} = [ \phi_0^{(D)} = 0.7,  \phi_1^{(D)} = 0.6, \sigma^2_{(D)} = 0.6, \phi_1^{(I)} = 0.3, \sigma^2_{(I)} = 0.2, \phi_1^{(P)} = 0.8]^T$
\par
\end{frame}

\begin{frame}
\frametitle{Hierarchical Dynamic Poisson Realization}
\framesubtitle{Components}
\centering
\includegraphics[scale=0.45]{hdpm-log-param}\\
\bigskip
\small
$\boldsymbol{\theta} = [ \phi_0^{(D)} = 0.7,  \phi_1^{(D)} = 0.6, \sigma^2_{(D)} = 0.6, \phi_1^{(I)} = 0.3, \sigma^2_{(I)} = 0.2, \phi_1^{(P)} = 0.8]^T$
\par
\end{frame}

%%% Latent State Inference %%%
\begin{frame}
\frametitle{Latent State Inference}
\framesubtitle{SIR particle filter}
\centering
\includegraphics[scale=0.45]{hdpm-est}
\end{frame}

%%% Parameter Inference %%%
\begin{frame}
\frametitle{Parameter Inference}
\framesubtitle{Log-likelihood plots w.r.t. $\phi_1^{(D)}$}
\centering
\includegraphics[scale=0.30]{hdpm-loglik-Dphi1}
\includegraphics[scale=0.30]{hdpm-loglik-Dphi1-zoom}
\end{frame}

\begin{frame}
\frametitle{Parameter Inference}
\framesubtitle{Log-likelihood plots w.r.t. $\sigma_{(D)}^2$}
\centering
\includegraphics[scale=0.30]{hdpm-loglik-Dvar}
\includegraphics[scale=0.30]{hdpm-loglik-Dvar-zoom}
\end{frame}

\begin{frame}
\frametitle{Parameter Inference}
\framesubtitle{Results}
\begin{table}
\centering
\begin{tabular}{l r r r r r r}
\hline
& $\phi_0^{(D)}$ &  $\phi_1^{(D)}$ & $\sigma^2_{(D)}$ & $\phi_1^{(I)}$ & $\sigma^2_{(I)}$ & $\phi_1^{(P)}$\\
\hline
True        & 0.70  & 0.60 &  0.30 & 0.80 & 0.60 & 0.20\\
SIR         & 0.76  & 0.56  & 0.85 & 0.48 & 0.83 & 1.13\\
IS            & 0.65  & 0.59  & 0.40 & 0.63 & 0.35 & 0.31\\
\hline
\end{tabular}
\end{table}
\end{frame}

%%%%%%%
%      QA     %
%%%%%%%
\section{Q \& A}

\end{document}