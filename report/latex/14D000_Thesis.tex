\documentclass[11pt, oneside]{scrreprt}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{bm}
\usepackage{subcaption}
\usepackage{graphicx}
\graphicspath{ {images/} }


%%%%%%%%
%    Cover     %
%%%%%%%%
\title{\textit{}\\\textit{}\\State Space Models}
\author{Hans-Peter H{\"o}llwirth}
\publishers{Master Project Report \\ 
Barcelona Graduate School of Economics \\ Master Degree in Data Science \\ 2017}
\date{}

\begin{document}
\maketitle
%\afterpage{\blankpage}

%%%%%%%%%%%%%
%    Table of Contents   %
%%%%%%%%%%%%%
\newpage
\tableofcontents
\newpage

%%%%%%%%%%
%    Introduction   %
%%%%%%%%%%
\chapter{Introduction}
\label{chp:introduction}



%%%%%%%%
%    Models   %
%%%%%%%%
\chapter{State Space Models}
\label{chp:models}
State space models consist of two set of data:
\begin{enumerate}
	\item A series of \textbf{latent states} $\{x_t\}_{t=1}^T$ (with $x_t \in \mathcal{X}$) that forms a Markov chain. Thus, $x_t$ is independent of all past states but $x_{t-1}$.
	\item A set of \textbf{observations} $\{y_t\}_{t=1}^T$ (with $y_t \in \mathcal{Y}$) where any observation $y_t$ only depends on its latent state $x_t$. In other words, an observation is a noisy representation of its underlying state.
\end{enumerate}
Note that if the state space $\mathcal{X}$ and the observation state $\mathcal{Y}$ are both discrete sets, the state space model reduces to a Hidden Markov Model.\\

The relation between the latent states and observations can be summarized by two probability distributions:
\begin{enumerate}
	\item The \textbf{transition density} from the current state to a new state $p(x_{t+1} | x_t, \boldsymbol{\theta})$.
	\item The \textbf{measurement density} for an observation given the latent state $p(y_t | x_t, \boldsymbol{\theta})$.
\end{enumerate}
Here, $\boldsymbol{\theta} \in \Theta$ denotes the parameter vector of a state space model. 


%%%%  Local Level Model  %%%%
\section{Local Level Model}
Arguably, the simplest state space model is the (univariate) local level model. It has the following form:

\begin{center}
\begin{tabular}{ r r l }
  observation: & $y_t = x_t + \epsilon_t$, & $\epsilon_t \sim N(0,\sigma_{\epsilon}^2)$ \\
  state: & $x_{t+1} = x_t + \eta_t$, & $\eta_t \sim N(0,\sigma_{\eta}^2)$ \\
\end{tabular}
\end{center}
\bigskip
with some initial state $x_1 \sim N(a_1, P_1)$. All noise elements, i.e. all $\epsilon_t$'s and $\eta_t$'s, are both mutually independent and independent from the initial state $x_1$. Assuming that we know $a_1$ and $P_1$, the model is fully specified by the following vector of parameters:
$$
\boldsymbol{\theta} = [\sigma_{\eta}^2,  \sigma_{\epsilon}^2]^T
$$
Note that in the case of noise-free observations (i.e. $\sigma_{\eta}^2 = 0$), the model reduces to a pure random-walk. Likewise, if $\sigma_{\epsilon}^2 = 0$, the observations $\{y_t\}_{t=1}^T$ are a white noise representation of a some value $x_1$.\\
The transition and measurement density of the local level model are simple to deduce:
\begin{center}
\begin{tabular}{ r l }
  $p(x_{t+1} | x_t, \boldsymbol{\theta})$ & $\sim N(x_t,\sigma_{\epsilon}^2)$ \\
  $p(y_t | x_t, \boldsymbol{\theta})$ & $\sim N(x_t,\sigma_{\eta}^2)$ \\
\end{tabular}
\end{center}
\bigskip

%%%%  Latent State Inference  %%%%
\section{Latent State Inference}
Often, the main objective in state space models is to infer the latent state from observations. Let $\mathcal{I}_t$ denote the set of observed values up to time $t$:
$$
\mathcal{I}_t = \{y_1, y_2, \ldots, y_t\}
$$ 
Then information about the latent state $x_t$ can be summarized by the following two probability distributions:
\begin{enumerate}
	\item The \textbf{prediction density}, $p(x_{t} | \mathcal{I}_{t-1}, \boldsymbol{\theta})$, gives the probability of $x_t$ given past observations  $\mathcal{I}_{t-1}$.
	\item The \textbf{filtering density}, $p(x_{t} | \mathcal{I}_{t}, \boldsymbol{\theta})$, gives the probability of $x_t$ given the current and past observations  $\mathcal{I}_{t}$.
\end{enumerate}	
The prediction and filtering densities are recursively related. Given the filtering density for state $x_{t-1}$, the prediction density for state $x_t$ is
$$
p(x_{t} | \mathcal{I}_{t-1}, \boldsymbol{\theta}) = \int p(x_t | x_{t-1}, \boldsymbol{\theta}) p(x_{t-1} | \mathcal{I}_{t-1}, \boldsymbol{\theta}) dx_{t-1}
$$
where the first term in the integral is the transition density from $x_{t-1}$ to $x_t$, and the second term is the filtering density from before. Likewise, given the prediction density for state $x_t$, the filtering density for $x_t$ is
$$
p(x_{t} | \mathcal{I}_{t}, \boldsymbol{\theta}) = \int p(x_t | x_{t-1}, \boldsymbol{\theta}) p(x_{t-1} | \mathcal{I}_{t-1}, \boldsymbol{\theta}) dx_{t-1}
$$

%%%%  Parameter Inference  %%%%
\section{Parameter Inference}
Assuming a particular state space model, another common objective is is to infer the model parameters from observations. This is usually achieved via \textbf{maximum likelihood estimation}. The log-likelihood of the observations for a given parameter vector $\boldsymbol{\theta}$ is the product of the conditional densities of observations, given all previous observations:
\begin{align} 
\begin{split}
\log \mathcal{L}(\boldsymbol{\theta}) &= \log \prod_{t=1}^T p(y_t | \mathcal{I}_{t-1}, \boldsymbol{\theta})\\
&= \sum_{t=1}^T \log  p(y_t | \mathcal{I}_{t-1}, \boldsymbol{\theta})\\
&= \sum_{t=1}^T \log  \int p(y_t | x_{t}, \boldsymbol{\theta}) p(x_{t} | \mathcal{I}_{t-1}, \boldsymbol{\theta}) d x_t\\
\end{split}					
\end{align} 
The decomposition of the observation densities into measurement density and prediction density, however, makes the maximization problem analytically intractable.



%%%%%%%
%  Filtering %
%%%%%%%
\chapter{Filtering}
\label{chp:filtering}
The objective of filtering is to update our knowledge of the system each time a new observation $y_t$ is brought in. That is, we want to compute $x_t | y_t$


%%%%  Kalman Filter   %%%%
\section{Kalman Filter}
The Kalman filter calculates the mean and variance of the unobserved state, given the observations.

\subsection{Algorithm}
The filter is a recursive algorithm. The current best estimate is updated whenever a new observation is obtained. To start the recursion, we need an initial state which is drawn with $a_1$ and $P_1$. We assume $a_1$ and $P_1$ to be known. There are various ways to initialize the algorithm when $a_1$ and $P_1$ are unknown, however, these methods are beyond the scope of this project.

\subsection{Likelihood evaluation}
$$
\log L(Y_n) = -\frac{nd}{2} \log(2 \pi) - \frac{1}{2} \sum_{t=1}^T (\log |F_t| + v_t^TF_t^{-1} v_t)
$$

%\begin{align} 
%\begin{split}
%|m-M| &= |\mathbb{E}(X)-M|\\
%\end{split}					
%\end{align} 


%%%%  Particle Filter   %%%%
\section{Particle Filter}


%%%%  Importance Sampling Particle Filter   %%%%
\section{Importance Sampling Particle Filter}


%%%%%%%
%  Illustration %
%%%%%%%
\chapter{Illustration}
\label{chp:Illustration}


%%%%  Trivariate Local Level Model  %%%%
\section{Trivariate Local Level Model}

\subsection{The Model}
Consider a time series of length $T$ with each observation $\boldsymbol{y}_t=[y_{1t}, y_{2t}, y_{3t}]^T$ and each state $\boldsymbol{x}_t=[x_{1t}, x_{2t}, x_{3t}]^T$ being described by a 3-dimensional vector.

\bigskip
\begin{center}
\begin{tabular}{ r r l }
  observation: & $\boldsymbol{y}_t = \boldsymbol{x}_t + \boldsymbol{\epsilon}_t$, & $\boldsymbol{\epsilon}_t \sim N(\textbf{0}, \sigma_{\epsilon}^2 I_3)$ \\
  state: & $\boldsymbol{x}_{t+1} = \boldsymbol{x}_t + \boldsymbol{\eta}_t$, & $\boldsymbol{\eta}_t \sim N(\textbf{0}, \Sigma_{\eta})$ \\
\end{tabular}
\end{center}
\bigskip
with initial state $\boldsymbol{x}_1 \sim N(\boldsymbol{a}_1, P_1)$ and where we restrict the covariance matrix of the state disturbances, $\Sigma_{\eta}$, to the form
$$
\Sigma_{\eta} = 
\begin{bmatrix}
\sigma_{\eta 1}^2 & \rho \sigma_{\eta 1} \sigma_{\eta 2} & \rho \sigma_{\eta 1} \sigma_{\eta 3}
\\ \rho \sigma_{\eta 1} \sigma_{\eta 2} & \sigma_{\eta 2}^2 & \rho \sigma_{\eta 2} \sigma_{\eta 3}
\\ \rho \sigma_{\eta 1} \sigma_{\eta 3} & \rho \sigma_{\eta 2} \sigma_{\eta 3} & \sigma_{\eta 3}^2
\end{bmatrix}
$$
Thus, $\Sigma_{\eta}$ can be described by $\sigma_{\eta 1}^2$, $\sigma_{\eta 2}^2$, $\sigma_{\eta 3}^2 > 0$ and $\rho \in [0,1]$. Furthermore, we assume for simplicity that the observation noise has the same variance in each dimension $\sigma_{\epsilon}^2 > 0$. Therefore, the model is fully specified by the following vector of parameters:
$$
\boldsymbol{\theta} = [\rho, \sigma_{\eta 1}^2, \sigma_{\eta 2}^2, \sigma_{\eta 3}^2, \sigma_{\epsilon}^2]^T
$$
The initial state parameters $\boldsymbol{a}_1$ and $P_1$ are assumed to be known.

\subsection{Realization}
\textit{Figure \ref{fig:mllm_realization}} plots the states and observations for a realization of the trivariate local level model with length $T=100$. The model parameters are 
$$
\boldsymbol{\theta} = [\rho = 0.7, \sigma_{\eta 1}^2 = 4.2, \sigma_{\eta 2}^2 = 2.8, \sigma_{\eta 3}^2 = 0.9, \sigma_{\epsilon}^2 = 1.0]^T
$$
The initial state $x_1$ is drawn from a standard normal.

\begin{figure}[h!]
\centering
\includegraphics[width=115mm]{../../images/mllm-realization.png}
\caption{Realization of the model with $T=100$}
\label{fig:mllm_realization}
\end{figure}




%%%%  Hierarchical Dynamic Poisson Model   %%%%
\newpage
\section{Hierarchical Dynamic Poisson Model}
Explain the main idea and potential use cases.

\subsection{The Model}
Consider a time series over $M$ days, each consisting of $N$ intra-daily observations. 
Let $m$ denote the day and $n$ be the intraday index.

\bigskip
\begin{center}
\begin{tabular}{ r r l }
  observation: & $y_{mn}$ & $= \text{Poisson}(\lambda_{mn})$\\
  state: & $\log \lambda_{mn}$ & $= \log \lambda_m^{(D)} + \log \lambda_{mn}^{(I)} + \log \lambda_n^{(P)}$\\  
\end{tabular}
\end{center}
\bigskip
where the state consists of a daily, an intra-daily, and a periodic component:
\bigskip
\begin{center}
\begin{tabular}{ r l l }
  daily component: & $\log \lambda_{m+1}^{(D)} = \phi_0^{(D)} + \phi_1^{(D)} \log \lambda_{m}^{(D)}  + \eta_m^{(D)}$ & $\eta_t \sim N(0, \sigma^2_{(D)})$ \\
  intra-daily component: & $\log \lambda_{mn+1}^{(I)} = \phi_1^{(I)} \log \lambda_{mn}^{(I)}  + \eta_{mn}^{(I)}$ & $\eta_{mn} \sim N(0, \sigma^2_{(I)})$ \\
    periodic component: & $\log \lambda_n^{(P)} = \phi_1^{(P)} \sin(\pi (n-1)/M)$ &\\
\end{tabular}
\end{center}
\bigskip
The initial daily and intra-daily component is drawn from a normal with mean $a_1$ and covariance $P_1$: 
$$\log \lambda_{1}^{(D)}, \log \lambda_{1}^{(I)}  \sim N(a_1, P_1)$$ 
Note that both the daily and intra-daily component constitute an AR(1) model, with the mean of the intra-daily component $\phi_0^{(I)}$ set to 0. 
The model is fully specified by the following vector of parameters:
$$
\boldsymbol{\theta} = [ \phi_0^{(D)},  \phi_1^{(D)}, \sigma^2_{(D)}, \phi_1^{(I)}, \sigma^2_{(I)}, \phi_1^{(P)}]^T
$$
Again, the initial state parameters $a_1$ and $P_1$ are assumed to be known.


\subsection{Realization}
\textit{Figure \ref{fig:hdpm_realization}} plots the states and observations for a realization of the hierarchical dyanmic Poisson model over $N=5$ days with $M=20$ intra-daily observations. The model parameters are 
$$
\boldsymbol{\theta} = [ \phi_0^{(D)} = 0.7,  \phi_1^{(D)} = 0.6, \sigma^2_{(D)} = 0.6, \phi_1^{(I)} = 0.3, \sigma^2_{(I)} = 0.2, \phi_1^{(P)} = 0.8]^T
$$
The initial daily and intra-daily state components were drawn from a standard normal.

\begin{figure}[h!]
\centering
\includegraphics[width=115mm]{../../images/hdpm-realization.png}
\caption{Realization of the model with $N=5$ and $M=20$}
\label{fig:hdpm_realization}
\end{figure}


\subsection{Densities}
State transition and prediction density and how they are used in the particle filter

\subsection{Maximum Likelihood Estimation}
Show log-likelihood plots

\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=70mm]{../../images/hdpm-loglik-Dphi0.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=70mm]{../../images/hdpm-loglik-Dphi1.png}
\end{subfigure}
\caption{Belief convergence without misinformation after 300 and 2000 iterations}
\label{fig:conv_nomisinfo}
\end{figure}


%%%%%%%%%%
%    Conclusion   %
%%%%%%%%%%
\chapter{Conclusion}
\label{chp:conclusion}
by Etessami et al.\cite{etessami2014_2}

%%%%%%%%%%%
%    Bibliography          %
%%%%%%%%%%%
%\afterpage{\blankpage}
\bibliography{references}
\bibliographystyle{plain}
\end{document}  















